{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis\n",
    "\n",
    "We use the movie reviews from the IMDb dataset to classify film reviews as either **positive** or **negative**. \n",
    "\n",
    "In **Part 1**, we use a Gradient-Boosted Decision Tree classifier to classify Bag of Words (BoW).\n",
    "\n",
    "In **Part 2**, we use RNNs with a long short-term memory (LSTM) layer to make use of words order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import sklearn.preprocessing as pr\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data\n",
    "\n",
    "We use movie reviews from the IMDb database. Keras has a built-in [IMDb movie reviews dataset](https://keras.io/datasets/#imdb-movie-reviews-sentiment-classification) that we can use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset with 25000 training samples, 25000 test samples\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import imdb\n",
    "\n",
    "# Set the vocabulary size -- i.e. exclude the least frequent words so that we only have 5000 \n",
    "vocabulary_size = 5000\n",
    "\n",
    "# Load in training and test data\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=vocabulary_size)\n",
    "print(\"Loaded dataset with {} training samples, {} test samples\".format(len(X_train), len(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Review ---\n",
      "[1, 785, 189, 438, 47, 110, 142, 7, 6, 2, 120, 4, 236, 378, 7, 153, 19, 87, 108, 141, 17, 1004, 5, 2, 883, 2, 23, 8, 4, 136, 2, 2, 4, 2, 43, 1076, 21, 1407, 419, 5, 2, 120, 91, 682, 189, 2818, 5, 9, 1348, 31, 7, 4, 118, 785, 189, 108, 126, 93, 2, 16, 540, 324, 23, 6, 364, 352, 21, 14, 9, 93, 56, 18, 11, 230, 53, 771, 74, 31, 34, 4, 2834, 7, 4, 22, 5, 14, 11, 471, 9, 2, 34, 4, 321, 487, 5, 116, 15, 2, 4, 22, 9, 6, 2286, 4, 114, 2679, 23, 107, 293, 1008, 1172, 5, 328, 1236, 4, 1375, 109, 9, 6, 132, 773, 2, 1412, 8, 1172, 18, 2, 29, 9, 276, 11, 6, 2768, 19, 289, 409, 4, 2, 2140, 2, 648, 1430, 2, 2, 5, 27, 3000, 1432, 2, 103, 6, 346, 137, 11, 4, 2768, 295, 36, 2, 725, 6, 3208, 273, 11, 4, 1513, 15, 1367, 35, 154, 2, 103, 2, 173, 7, 12, 36, 515, 3547, 94, 2547, 1722, 5, 3547, 36, 203, 30, 502, 8, 361, 12, 8, 989, 143, 4, 1172, 3404, 10, 10, 328, 1236, 9, 6, 55, 221, 2989, 5, 146, 165, 179, 770, 15, 50, 713, 53, 108, 448, 23, 12, 17, 225, 38, 76, 4397, 18, 183, 8, 81, 19, 12, 45, 1257, 8, 135, 15, 2, 166, 4, 118, 7, 45, 2, 17, 466, 45, 2, 4, 22, 115, 165, 764, 2, 5, 1030, 8, 2973, 73, 469, 167, 2127, 2, 1568, 6, 87, 841, 18, 4, 22, 4, 192, 15, 91, 7, 12, 304, 273, 1004, 4, 1375, 1172, 2768, 2, 15, 4, 22, 764, 55, 2, 5, 14, 4233, 2, 4, 1375, 326, 7, 4, 4760, 1786, 8, 361, 1236, 8, 989, 46, 7, 4, 2768, 45, 55, 776, 8, 79, 496, 98, 45, 400, 301, 15, 4, 1859, 9, 4, 155, 15, 66, 2, 84, 5, 14, 22, 1534, 15, 17, 4, 167, 2, 15, 75, 70, 115, 66, 30, 252, 7, 618, 51, 9, 2161, 4, 3130, 5, 14, 1525, 8, 2, 15, 2, 165, 127, 1921, 8, 30, 179, 2532, 4, 22, 9, 906, 18, 6, 176, 7, 1007, 1005, 4, 1375, 114, 4, 105, 26, 32, 55, 221, 11, 68, 205, 96, 5, 4, 192, 15, 4, 274, 410, 220, 304, 23, 94, 205, 109, 9, 55, 73, 224, 259, 3786, 15, 4, 22, 528, 1645, 34, 4, 130, 528, 30, 685, 345, 17, 4, 277, 199, 166, 281, 5, 1030, 8, 30, 179, 4442, 444, 2, 9, 6, 371, 87, 189, 22, 5, 31, 7, 4, 118, 7, 4, 2068, 545, 1178, 829]\n",
      "--- Label ---\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# Inspect a sample review and its label\n",
    "example_review_idx = 10\n",
    "\n",
    "print(\"--- Review ---\")\n",
    "print(X_train[example_review_idx])\n",
    "print(\"--- Label ---\")\n",
    "print(y_train[example_review_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that reviews are already processed to be separated in words, and each word is mapped to an integer. Labels are binary (0 for **negative**, 1 for **positive**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Review (with words) ---\n",
      "['the', 'clear', 'fact', 'entertaining', 'there', 'life', 'back', 'br', 'is', 'and', 'show', 'of', 'performance', 'stars', 'br', 'actors', 'film', 'him', 'many', 'should', 'movie', 'reasons', 'to', 'and', 'reading', 'and', 'are', 'in', 'of', 'scenes', 'and', 'and', 'of', 'and', 'out', 'compared', 'not', 'boss', 'yes', 'to', 'and', 'show', 'its', 'disappointed', 'fact', 'raw', 'to', 'it', 'justice', 'by', 'br', 'of', 'where', 'clear', 'fact', 'many', 'your', 'way', 'and', 'with', 'city', 'nice', 'are', 'is', 'along', 'wrong', 'not', 'as', 'it', 'way', 'she', 'but', 'this', 'anything', 'up', \"haven't\", 'been', 'by', 'who', 'of', 'choices', 'br', 'of', 'you', 'to', 'as', 'this', \"i'd\", 'it', 'and', 'who', 'of', 'shot', \"you'll\", 'to', 'love', 'for', 'and', 'of', 'you', 'it', 'is', 'sequels', 'of', 'little', 'quest', 'are', 'seen', 'watched', 'front', 'chemistry', 'to', 'simply', 'alive', 'of', 'chris', 'being', 'it', 'is', 'say', 'easy', 'and', 'cry', 'in', 'chemistry', 'but', 'and', 'all', 'it', 'maybe', 'this', 'is', 'wing', 'film', 'job', 'live', 'of', 'and', 'relief', 'and', 'level', 'names', 'and', 'and', 'to', 'be', 'stops', 'serial', 'and', 'watch', 'is', 'men', 'go', 'this', 'of', 'wing', 'american', 'from', 'and', 'moving', 'is', 'accepted', 'put', 'this', 'of', 'jerry', 'for', 'places', 'so', 'work', 'and', 'watch', 'and', 'lot', 'br', 'that', 'from', 'sometimes', 'wondered', 'make', 'department', 'introduced', 'to', 'wondered', 'from', 'action', 'at', 'turns', 'in', 'low', 'that', 'in', 'gay', \"i'm\", 'of', 'chemistry', 'bible', 'i', 'i', 'simply', 'alive', 'it', 'is', 'time', 'done', 'inspector', 'to', 'watching', 'look', 'world', 'named', 'for', 'more', 'tells', 'up', 'many', 'fans', 'are', 'that', 'movie', 'music', 'her', 'get', 'grasp', 'but', 'seems', 'in', 'people', 'film', 'that', 'if', 'explain', 'in', 'why', 'for', 'and', 'find', 'of', 'where', 'br', 'if', 'and', 'movie', 'throughout', 'if', 'and', 'of', 'you', 'best', 'look', 'red', 'and', 'to', 'recently', 'in', 'successfully', 'much', 'unfortunately', 'going', 'dan', 'and', 'stuck', 'is', 'him', 'sequences', 'but', 'of', 'you', 'of', 'enough', 'for', 'its', 'br', 'that', 'beautiful', 'put', 'reasons', 'of', 'chris', 'chemistry', 'wing', 'and', 'for', 'of', 'you', 'red', 'time', 'and', 'to', 'as', 'companion', 'and', 'of', 'chris', 'less', 'br', 'of', 'subplots', 'torture', 'in', 'low', 'alive', 'in', 'gay', 'some', 'br', 'of', 'wing', 'if', 'time', 'actual', 'in', 'also', 'side', 'any', 'if', 'name', 'takes', 'for', 'of', 'friendship', 'it', 'of', '10', 'for', 'had', 'and', 'great', 'to', 'as', 'you', 'students', 'for', 'movie', 'of', 'going', 'and', 'for', 'bad', 'well', 'best', 'had', 'at', 'woman', 'br', 'musical', 'when', 'it', 'caused', 'of', 'gripping', 'to', 'as', 'gem', 'in', 'and', 'for', 'and', 'look', 'end', 'gene', 'in', 'at', 'world', 'aliens', 'of', 'you', 'it', 'meet', 'but', 'is', 'quite', 'br', 'western', 'ideas', 'of', 'chris', 'little', 'of', 'films', 'he', 'an', 'time', 'done', 'this', 'were', 'right', 'too', 'to', 'of', 'enough', 'for', 'of', 'ending', 'become', 'family', 'beautiful', 'are', 'make', 'right', 'being', 'it', 'time', 'much', 'bit', 'especially', 'craig', 'for', 'of', 'you', 'parts', 'bond', 'who', 'of', 'here', 'parts', 'at', 'due', 'given', 'movie', 'of', 'once', 'give', 'find', 'actor', 'to', 'recently', 'in', 'at', 'world', 'dolls', 'loved', 'and', 'it', 'is', 'video', 'him', 'fact', 'you', 'to', 'by', 'br', 'of', 'where', 'br', 'of', 'grown', 'fight', 'culture', 'leads']\n",
      "--- Label ---\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# Map word IDs back to words\n",
    "word2id = imdb.get_word_index() \n",
    "id2word = {i: word for word, i in word2id.items()}\n",
    "\n",
    "print(\"--- Review (with words) ---\")\n",
    "print([id2word.get(i, \" \") for i in X_train[example_review_idx]])\n",
    "print(\"--- Label ---\")\n",
    "print(y_train[example_review_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pad sequences\n",
    "\n",
    "Each review can have a different number of words. Thus, we pad them to have a max number of words. This is mostly needed for Part 2, where each review is used as an input to the RNN, word after word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the maximum number of words per document\n",
    "max_words = 500\n",
    "\n",
    "# Pad sequences in X_train and X_test\n",
    "X_train = sequence.pad_sequences(\n",
    "    X_train, maxlen=500, dtype=\"int32\", padding=\"pre\", truncating=\"pre\", value=0.0)\n",
    "\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=500, dtype=\"int32\", padding=\"pre\", truncating=\"pre\", value=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1 - Classify Bag of Words using grandient-boosted decision trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Extract BoW features\n",
    "\n",
    "Note that the 'words' are encoded in integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_BoW_features(words_train, words_test, vocabulary_size=5000):\n",
    "    \"\"\"Extract Bag-of-Words for a given set of documents\"\"\"\n",
    "    \n",
    "    # Fit a vectorizer to training documents and use it to transform them\n",
    "    vectorizer = CountVectorizer(max_features=vocabulary_size,\n",
    "            preprocessor=lambda x: x, tokenizer=lambda x: x) \n",
    "    features_train = vectorizer.fit_transform(words_train).toarray()\n",
    "\n",
    "    # Apply the same vectorizer to transform the test documents (ignore unknown words)\n",
    "    features_test = vectorizer.transform(words_test).toarray()\n",
    "\n",
    "    vocabulary = vectorizer.vocabulary_\n",
    "    \n",
    "    return features_train, features_test, vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Bag of Words features for both training and test datasets\n",
    "features_train, features_test, vocabulary = extract_BoW_features(X_train, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalize BoW features in training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train = pr.normalize(features_train, axis=1)\n",
    "features_test = pr.normalize(features_test, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification using a Gradient-Boosted Decision Tree classifier\n",
    "\n",
    "Tree-based algorithms are often used on BoW to leverage the features' sparsity. Howver, they might require some hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# more basic alternative to cross-validation\n",
    "def split_train_validation(X_train, y_train, fraction_valid = 0.2):\n",
    "    \n",
    "    idx_split = int(len(y_train) * fraction_valid)\n",
    "    X_valid = X_train[:idx_split]\n",
    "    y_valid = y_train[:idx_split]\n",
    "    \n",
    "    X_train = X_train[idx_split:]\n",
    "    y_train = y_train[idx_split:]\n",
    "    \n",
    "    return X_train, y_train, X_valid, y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_gboost(X_train, X_valid, X_test, y_train, y_valid, y_test):  \n",
    "\n",
    "    best_model = None\n",
    "    best_score_valid = 0\n",
    "    \n",
    "    for n_estimators in [16, 32, 64]:\n",
    "        for learning_rate in [0.6, 0.8, 1]: \n",
    "\n",
    "            clf = GradientBoostingClassifier(n_estimators=n_estimators, learning_rate=learning_rate, \n",
    "                                             max_depth=1, random_state=0)\n",
    "            clf.fit(X_train, y_train)\n",
    "            model_score = clf.score(X_valid, y_valid)\n",
    "            if model_score > best_score_valid:\n",
    "                model_score = best_score_valid\n",
    "                best_model = clf\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create validation set to test model performance \n",
    "features_train, labels_train, features_valid, labels_valid = split_train_validation(features_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find best model by optimizing hyperparameters\n",
    "best_model = classify_gboost(features_train, features_valid, features_test, labels_train, labels_valid, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method BaseEstimator.get_params of GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,\n",
       "                           learning_rate=1, loss='deviance', max_depth=1,\n",
       "                           max_features=None, max_leaf_nodes=None,\n",
       "                           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                           min_samples_leaf=1, min_samples_split=2,\n",
       "                           min_weight_fraction_leaf=0.0, n_estimators=64,\n",
       "                           n_iter_no_change=None, presort='deprecated',\n",
       "                           random_state=0, subsample=1.0, tol=0.0001,\n",
       "                           validation_fraction=0.1, verbose=0,\n",
       "                           warm_start=False)>"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parameters of the best model\n",
    "best_model.get_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GradientBoostingClassifier] Accuracy best model: train = 0.822, validation = 0.8124, test = 0.81588\n"
     ]
    }
   ],
   "source": [
    "# check performance on test set\n",
    "print(\"[{}] Accuracy best model: train = {}, validation = {}, test = {}\".format(\n",
    "    best_model.__class__.__name__,\n",
    "    best_model.score(features_train, labels_train),\n",
    "    best_model.score(features_valid, labels_valid),\n",
    "    best_model.score(features_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tree-based model already achieves a good performance, and hyperparameters selection is sufficient to prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = best_model.predict(features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Review ---\n",
      "[' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', 'the', 'thats', 'who', 'and', 'and', 'are', 'is', 'manner', 'positive', 'br', 'seen', 'budget', 'and', 'confusing', 'this', 'and', 'but', 'often', 'davies', 'br', 'of', 'seen', 'great', 'and', 'she', 'were', 'care', 'double', 'and', 'or', 'of', 'manner', 'it', 'so', \"i've\", 'to', 'time', 'trained', 'must', 'long', 'are', 'of', 'less', 'if', 'is', 'successful', 'in', 'allow', 'and', 'for', 'all', 'give', 'cold', 'to', 'jungle', 'this', 'as', 'track', 'must', 'americans', 'i', 'i', 'robin', 'and', 'it', 'of', 'killer', 'like', 'and', 'of', 'complete', 'white', 'to', 'if', 'scope', 'such', 'would', 'role', '4', 'of', 'semi', 'br', 'allow', 'and', 'film', 'about', 'accents', 'contain', 'to', 'and', 'sub', 'made', 'things', 'but', 'think', 'track', 'and', 'worthy', 'and', 'it', 'of', 'violence', \"you've\", 'war', 'there', 'forth', 'be', 'ending', 'less', 'to', 'it', 'years', 'never', 'and', 'movie', 'is', 'him', 'actually', 'crappy', 'which', 'split', 'are', 'of', 'and', 'ever', 'in', 'this', 'and', 'all', 'real', 'at', 'needless', 'marriage', 'and', 'role', 'of', 'less', 'br', 'and', 'confusing', 'in', 'davies', 'thinking', 'br', 'while', 'and', 'and', 'i', 'i', 'br', 'screen', 'dvd', 'to', 'last', 'well', 'ago', 'no', 'violence', 'whale', 'and', 'games', 'be', 'flying', 'and', 'film', 'and', 'front', 'but', \"don't\", 'of', 'little', 'noted', 'to', 'if', 'spend', 'an', 'of', 'too', \"i'm\", 'lou', 'brings', 'br', 'older', 'to', 'and', 'camera', 'of', 'too', 'movie', 'much', 'movie', 'is', 'very', 'wait', 'br', 'brought', 'i', 'i', 'but', 'and', 'br', 'simply', 'must', 'as', 'by', 'it', 'and', 'in', 'wonder']\n",
      "--- Predicted Label ---\n",
      "1\n",
      "--- True Label ---\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# check one example in test set\n",
    "print(\"--- Review ---\")\n",
    "print([id2word.get(i, \" \") for i in X_test[example_review_idx]])\n",
    "print(\"--- Predicted Label ---\")\n",
    "print(y_pred[example_review_idx])\n",
    "print(\"--- True Label ---\")\n",
    "print(y_test[example_review_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2 - Using RNNs \n",
    "\n",
    "The main difference with respect to the BoW approach is that we make use of the sequence of words present in each review, and not just look at the frequency of occurrence of given words in positive / negative reviews. \n",
    "\n",
    "We make use of a Long Short-Term Memory (**LSTM**) layer to use the order of words in the reviews for training, which could be crucial! (imagine the difference between: **\"The movie was better than expected\"** vs **\"I expected the movie to be better\"**). \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a RNN architecture\n",
    "\n",
    "We define our network using words embeddings and an LSTM layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 500, 32)           160000    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 213,301\n",
      "Trainable params: 213,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "embedding_size = 32\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocabulary_size, embedding_size, input_length=max_words))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/robyeva/miniconda3/envs/ranking_env/lib/python3.7/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/robyeva/miniconda3/envs/ranking_env/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 24936 samples, validate on 64 samples\n",
      "Epoch 1/3\n",
      "24936/24936 [==============================] - 127s 5ms/step - loss: 0.5207 - accuracy: 0.7365 - val_loss: 0.3065 - val_accuracy: 0.8438\n",
      "Epoch 2/3\n",
      "24936/24936 [==============================] - 134s 5ms/step - loss: 0.3078 - accuracy: 0.8735 - val_loss: 0.2215 - val_accuracy: 0.9062\n",
      "Epoch 3/3\n",
      "24936/24936 [==============================] - 149s 6ms/step - loss: 0.2462 - accuracy: 0.9031 - val_loss: 0.2670 - val_accuracy: 0.9219\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f8bf86416d0>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Specify training parameters: batch size and number of epochs\n",
    "batch_size = 64 ## now many input to show to the network before the parameters are updated and error is computed\n",
    "num_epochs = 3 ## number of times the network will go through the whole training set\n",
    "\n",
    "# Split in training and validation set \n",
    "X_valid, y_valid = X_train[:batch_size], y_train[:batch_size]  # first batch_size samples\n",
    "X_train2, y_train2 = X_train[batch_size:], y_train[batch_size:]  # rest for training\n",
    "\n",
    "model.fit(X_train2, y_train2,\n",
    "          validation_data=(X_valid, y_valid),\n",
    "          batch_size=batch_size, epochs=num_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.8710399866104126\n"
     ]
    }
   ],
   "source": [
    "# Evaluate your model on the test set\n",
    "scores = model.evaluate(X_test, y_test, verbose=0) \n",
    "\n",
    "print(\"Test accuracy:\", scores[1]) # first metric, so accuracy (see model.compile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RNN model achieves a better performance than the decision tree: as expected, making use of the words sequences seems to improve the classification results. \n",
    "\n",
    "Results could be further improved by tuning the batch size / #epochs parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally can save the model, to load it in future\n",
    "model_file = \"rnn_model.h5\"  # HDF5 file\n",
    "model.save(model_file)\n",
    "\n",
    "# Load model using keras.models.load_model()\n",
    "#from keras.models import load_model\n",
    "#model = load_model(model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Disclaimer: This notebook is inspired by a project which is part of the Natural language Processing Udacity Nanodegree."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
