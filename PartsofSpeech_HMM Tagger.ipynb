{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part of Speech Tagging with Hidden Markov Models \n",
    "\n",
    "Using a Hidden Markov Model implementation from the [Pomegranate](http://pomegranate.readthedocs.io/) library to tag part of speech out of sentences from the Brown corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from IPython.core.display import HTML\n",
    "from itertools import chain\n",
    "from collections import Counter, defaultdict\n",
    "from pomegranate import State, HiddenMarkovModel, DiscreteDistribution\n",
    "import nltk\n",
    "from nltk import pos_tag, word_tokenize\n",
    "from nltk.corpus import brown\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1: Read and preprocess the dataset\n",
    "\n",
    "The dataset is a copy of the [Brown corpus](https://en.wikipedia.org/wiki/Brown_Corpus), available in NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /home/robyeva/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('The', 'AT'),\n",
       " ('Fulton', 'NP-TL'),\n",
       " ('County', 'NN-TL'),\n",
       " ('Grand', 'JJ-TL'),\n",
       " ('Jury', 'NN-TL'),\n",
       " ('said', 'VBD'),\n",
       " ('Friday', 'NR'),\n",
       " ('an', 'AT'),\n",
       " ('investigation', 'NN'),\n",
       " ('of', 'IN'),\n",
       " (\"Atlanta's\", 'NP$'),\n",
       " ('recent', 'JJ'),\n",
       " ('primary', 'NN'),\n",
       " ('election', 'NN'),\n",
       " ('produced', 'VBD'),\n",
       " ('``', '``'),\n",
       " ('no', 'AT'),\n",
       " ('evidence', 'NN'),\n",
       " (\"''\", \"''\"),\n",
       " ('that', 'CS'),\n",
       " ('any', 'DTI'),\n",
       " ('irregularities', 'NNS'),\n",
       " ('took', 'VBD'),\n",
       " ('place', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('brown')\n",
    "training_corpus = nltk.corpus.brown\n",
    "training_corpus.tagged_sents()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_splitting_function(data, train_test_split = 0.8):\n",
    "    \"\"\"Split the corpus in set of sentences belonging to train and test set. The train_test_split fraction determines \n",
    "    the size of the training set.\"\"\"\n",
    "    list_idx = list(range(len(data)))\n",
    "    random.shuffle(list_idx)\n",
    "    split = int(train_test_split * len(list_idx))\n",
    "\n",
    "    training_data = np.array(data)[list_idx[:split]]\n",
    "    testing_data = np.array(data)[list_idx[split:]]\n",
    "    \n",
    "    return training_data, testing_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57340"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_corpus.tagged_sents())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Hunting', 'VBG-HL'), ('rifles', 'NNS-HL'), (',', ',-HL'), (\"'61\", 'CD-HL')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/robyeva/miniconda3/envs/nlp_env/lib/python3.6/site-packages/ipykernel_launcher.py:8: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  \n",
      "/home/robyeva/miniconda3/envs/nlp_env/lib/python3.6/site-packages/ipykernel_launcher.py:9: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "data = training_corpus.tagged_sents()\n",
    "data = list(data)\n",
    "training_data, testing_data = train_test_splitting_function(data, train_test_split = 0.8)\n",
    "print(training_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 - Set up counts \n",
    "\n",
    "Counts are used to define the emission and transition probabilities. In short, the HMM tagger has one hidden state for each possible tag, and is parameterized by two distributions: the emission probabilties giving the conditional probability of observing a given **word** from each hidden state, and the transition probabilities giving the conditional probability of moving between **tags** during the sequence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pair_counts(data):\n",
    "    \"\"\"Return nested dictionary with the number of occurrences of a given (word, tag) combination.\n",
    "    E.g. pair_counts[NOUN][time] == # occurrences in data of word \"time\", tagged as NOUN.\n",
    "    \"\"\"\n",
    "    dic_data = {}\n",
    "    data_list = [item for sublist in data for item in sublist]\n",
    "    \n",
    "    for i in range(len(data_list)):   \n",
    "        try: \n",
    "            dic_data[data_list[i][1]][data_list[i][0]] += 1\n",
    "        except:\n",
    "            try: \n",
    "                dic_data[data_list[i][1]][data_list[i][0]] = 1\n",
    "            except:\n",
    "                dic_data[data_list[i][1]] = {}\n",
    "                dic_data[data_list[i][1]][data_list[i][0]] = 1\n",
    "    return dic_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unigram_counts(data):\n",
    "    \"\"\"Return a dictionary with the number of occurrences of the value in data (across the whole data).\n",
    "    E.g. unigram_dict[NOUN] == # words in the data tagged as NOUN.\n",
    "    \"\"\"\n",
    "    tags_list = [item[1] for sublist in data for item in sublist]\n",
    "    used = set()\n",
    "    unique = [x for x in tags_list if x not in used and (used.add(x) or True)]\n",
    "    unigram_dict = {}\n",
    "    for el in unique:\n",
    "        unigram_dict[el] = len([i for i, x in enumerate(tags_list) if x == el])\n",
    "    return unigram_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigram_counts(sequences):\n",
    "    \"\"\"Return a dictionary keyed to each unique pair of values with the number of occurrences of \n",
    "    successive tags.\n",
    "    E.g. bigram_dict[(NOUN, VERB)] == # occurrences of tag NOUN followed by tag VERB in sequences.\n",
    "    \"\"\"\n",
    "    biagram_dict = {}\n",
    "    for idx in range(len(sequences)):\n",
    "\n",
    "        c = sequences[idx]\n",
    "        # gives the tuples of consecutive tags\n",
    "        consec = [(c[i][1],c[i+1][1]) for i in range(0,len(c)-1)]\n",
    "        \n",
    "        for el in consec: \n",
    "            try: \n",
    "                biagram_dict[el] +=1\n",
    "            except: \n",
    "                biagram_dict[el] = 1\n",
    "    return biagram_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def starting_counts(sequences):\n",
    "    \"\"\"Return a dictionary that counts the number of occurrences of each tag at the beginning of any sentence in the sequence.\n",
    "    E.g. tag_dict[NOUN] == # starting words tagged as NOUN\n",
    "    \"\"\"\n",
    "    tag_dict = {}\n",
    "    for idx in range(len(sequences)):\n",
    "        # take start element only\n",
    "        try:\n",
    "            el = sequences[idx][0][1]\n",
    "        except:\n",
    "            continue\n",
    "        try: \n",
    "            tag_dict[el] +=1\n",
    "        except: \n",
    "            tag_dict[el] = 1\n",
    "    return tag_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ending_counts(sequences):\n",
    "    \"\"\"Return a dictionary that counts the number of occurrences of each tag at the end of any sentence in the sequence.\n",
    "    E.g. tag_dict[NOUN] == #words at the end of sentence tagged as NOUN\n",
    "\n",
    "    \"\"\"\n",
    "    tag_dict = {}\n",
    "    for idx in range(len(sequences)):\n",
    "        # take start element only\n",
    "        try:\n",
    "            el = sequences[idx][-1][1]\n",
    "        except:\n",
    "            continue\n",
    "        try: \n",
    "            tag_dict[el] +=1\n",
    "        except: \n",
    "            tag_dict[el] = 1\n",
    "    return tag_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate counts for training data\n",
    "emission_counts = pair_counts(training_data)\n",
    "tag_unigrams = unigram_counts(training_data)\n",
    "tag_bigrams = bigram_counts(np.array(training_data))\n",
    "tag_starts = starting_counts(np.array(training_data))\n",
    "tag_ends = ending_counts(np.array(training_data))\n",
    "train_counts = pair_counts(training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 - Set up the Hidden Markov Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are as many nodes as unique tags, plus a starting and an ending node. The **emission distribution** assigns a probability to each word, given a tag. The **transition probabilities** govern the transitions from one tag to another one (edges)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_model = HiddenMarkovModel(name=\"base-hmm-tagger\")\n",
    "states=dict()\n",
    "\n",
    "words = [item[0] for sublist in training_data for item in sublist]\n",
    "used = set()\n",
    "vocabulary = [x for x in words if x not in used and (used.add(x) or True)]\n",
    "\n",
    "tags_list = [item[1] for sublist in training_data for item in sublist]\n",
    "used = set()\n",
    "tags_unique = [x for x in tags_list if x not in used and (used.add(x) or True)]\n",
    "\n",
    "for t in tags_unique:\n",
    "    \n",
    "    # add emission probabilities P(w|t)\n",
    "    prob = {}\n",
    "    for i, w in enumerate(vocabulary):\n",
    "        try:\n",
    "            prob[w] = train_counts[t][w] / float(tag_unigrams[t])\n",
    "        except: \n",
    "            prob[w] = 0 \n",
    "    prob_emissions = DiscreteDistribution(prob)\n",
    "    \n",
    "    states[t] = State(prob_emissions, name=t)\n",
    "    basic_model.add_states(states[t])\n",
    "    \n",
    "    # add transition to the end state P(end|t)\n",
    "    try:\n",
    "        end_prob = tag_ends[t] / sum(tag_ends.values())\n",
    "    except:\n",
    "        end_prob = 0\n",
    "    basic_model.add_transition(states[t], basic_model.end, end_prob)\n",
    "    \n",
    "    # add transition to the start state P(t|start) \n",
    "    try:\n",
    "        start_prob = tag_starts[t] / sum(tag_starts.values())\n",
    "    except:\n",
    "        start_prob = 0\n",
    "    basic_model.add_transition(basic_model.start, states[t], start_prob)      \n",
    "\n",
    "# add transitions across states (from t1 to t2)\n",
    "for t1 in tag_unigrams.keys():\n",
    "    for t2 in tag_unigrams.keys():\n",
    "        try:\n",
    "            prob = tag_bigrams[t1,t2] / tag_unigrams[t1]\n",
    "        except:\n",
    "            prob = 0 \n",
    "        basic_model.add_transition(states[t1],states[t2], prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model\n",
    "basic_model.bake()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the model there are  348  states and  120408  edges\n"
     ]
    }
   ],
   "source": [
    "print(\"In the model there are \", basic_model.state_countcount(), \" states and \", basic_model.edge_count(), \" edges\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The state transition matrix, P(Xt|Xt-1):\n",
      "\n",
      "[[0.         0.00065399 0.00152599 ... 0.         0.03243809 0.        ]\n",
      " [0.         0.         0.11316063 ... 0.         0.0062867  0.00041444]\n",
      " [0.         0.         0.         ... 0.         0.00061644 0.00136266]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.00213936 0.         0.00092062]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# visualize transition matrix\n",
    "column_order = [\"base-hmm-tagger-start\"] + sorted(tags_unique) + ['base-hmm-tagger-end']  # Override the Pomegranate default order\n",
    "column_names = [s.name for s in basic_model.states]\n",
    "order_index = [column_names.index(c) for c in column_order]\n",
    "\n",
    "# re-order the rows/columns to match the alphabetic column order \n",
    "transitions = basic_model.dense_transition_matrix()[:, order_index][order_index, :]\n",
    "print(\"The state transition matrix, P(Xt|Xt-1):\\n\")\n",
    "print(transitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 - Model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_unknown(sequence, vocabulary):\n",
    "    \"\"\"Replace unknown words (i.e. not present in the vocabulary) by 'nan'. Pomegranate will ignore these values\n",
    "    during computation.\n",
    "    \"\"\"\n",
    "    return [w if w in vocabulary else 'nan' for w in sequence]\n",
    "\n",
    "def simplify_decoding(X, vocabulary, model):\n",
    "    \"\"\"X should be a 1-D sequence of observations for the model to predict. We use the Viterbi algortihm to choose the \n",
    "    path with highest probability\n",
    "    \"\"\"\n",
    "    _, state_path = model.viterbi(replace_unknown(X, vocabulary))\n",
    "    return [state[1].name for state in state_path[1:-1]]  # do not show the start/end state predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(X, Y, vocabulary, model):\n",
    "    \"\"\"Calculate the prediction accuracy.\n",
    "    X: list of words\n",
    "    Y: list of the true labels (tags)\n",
    "    Vocabulary: list of all the unique words in the corpus\n",
    "    Model: the baked HMM\n",
    "    \"\"\"\n",
    "    correct = total_predictions = 0\n",
    "    for observations, actual_tags in zip(X, Y):\n",
    "        try:\n",
    "            most_likely_tags = simplify_decoding(observations, vocabulary, model)\n",
    "            correct += sum(p == t for p, t in zip(most_likely_tags, actual_tags))\n",
    "        except:\n",
    "            pass\n",
    "        total_predictions += len(observations)\n",
    "    return correct / total_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate training and test set in lists of words and tags\n",
    "## Train set\n",
    "corpus_words_training = []\n",
    "for sentence in training_data:\n",
    "    word_list = tuple([item[0] for item in sentence])\n",
    "    corpus_words_training.append(word_list)\n",
    "\n",
    "corpus_tags_training = []\n",
    "for sentence in training_data:\n",
    "    t_list = tuple([item[1] for item in sentence])\n",
    "    corpus_tags_training.append(t_list)\n",
    "    \n",
    "## Test set\n",
    "corpus_words_test = []\n",
    "for sentence in testing_data:\n",
    "    word_list = tuple([item[0] for item in sentence])\n",
    "    corpus_words_test.append(word_list)\n",
    "\n",
    "corpus_tags_test = []\n",
    "for sentence in testing_data:\n",
    "    t_list = tuple([item[1] for item in sentence])\n",
    "    corpus_tags_test.append(t_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy HMM model: 97.48%\n",
      "testing accuracy HMM model: 89.43%\n"
     ]
    }
   ],
   "source": [
    "hmm_training_acc = accuracy(corpus_words_training, corpus_tags_training, vocabulary, basic_model)\n",
    "print(\"training accuracy HMM model: {:.2f}%\".format(100 * hmm_training_acc))\n",
    "\n",
    "hmm_testing_acc = accuracy(corpus_words_test, corpus_tags_test, vocabulary, basic_model)\n",
    "print(\"testing accuracy HMM model: {:.2f}%\".format(100 * hmm_testing_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 - Decoding sequences with the HMM Tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('It',\n",
       " 'is',\n",
       " 'puzzling',\n",
       " 'to',\n",
       " 'the',\n",
       " 'occidental',\n",
       " 'mind',\n",
       " '(',\n",
       " 'to',\n",
       " 'mine',\n",
       " 'at',\n",
       " 'least',\n",
       " ')',\n",
       " 'to',\n",
       " 'assign',\n",
       " '``',\n",
       " 'sacredness',\n",
       " \"''\",\n",
       " 'to',\n",
       " 'animal',\n",
       " ',',\n",
       " 'insect',\n",
       " ',',\n",
       " 'and',\n",
       " 'plant',\n",
       " 'life',\n",
       " '.')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_words_test[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: ('John', 'received', 'a', 'promotion', 'in', 'his', 'firm', '.')\n",
      "\n",
      "Predicted labels:\n",
      "-----------------\n",
      "['NP', 'VBD', 'AT', 'NN', 'IN', 'PP$', 'NN', '.']\n",
      "\n",
      "Actual labels:\n",
      "--------------\n",
      "('NP', 'VBD', 'AT', 'NN', 'IN', 'PP$', 'NN', '.')\n",
      "\n",
      "\n",
      "Sentence: ('He', 'conducted', 'it', 'with', 'less', 'diplomacy', 'and', 'more', 'spontaneous', 'violence', 'than', 'the', 'Sicilians', ',', 'but', 'he', 'had', 'his', 'huge', 'North', 'Side', 'portion', 'to', 'exploit', 'and', 'he', 'made', 'a', 'great', 'deal', 'of', 'money', '.')\n",
      "\n",
      "Predicted labels:\n",
      "-----------------\n",
      "['PPS', 'VBD', 'PPO', 'IN', 'AP', 'NN', 'CC', 'QL', 'JJ', 'NN', 'IN', 'AT', 'NN', ',', 'CC', 'PPS', 'HVD', 'PP$', 'JJ', 'JJ-TL', 'NN-TL', 'NN', 'TO', 'VB', 'CC', 'PPS', 'VBD', 'AT', 'JJ', 'NN', 'IN', 'NN', '.']\n",
      "\n",
      "Actual labels:\n",
      "--------------\n",
      "('PPS', 'VBD', 'PPO', 'IN', 'AP', 'NN', 'CC', 'AP', 'JJ', 'NN', 'CS', 'AT', 'NPS', ',', 'CC', 'PPS', 'HVD', 'PP$', 'JJ', 'JJ-TL', 'NN-TL', 'NN', 'TO', 'VB', 'CC', 'PPS', 'VBD', 'AT', 'JJ', 'NN', 'IN', 'NN', '.')\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx in range(2):\n",
    "    print(\"Sentence: {}\\n\".format(corpus_words_test[idx]))\n",
    "    print(\"Predicted labels:\\n-----------------\")\n",
    "    print(simplify_decoding(corpus_words_test[idx], vocabulary, basic_model))\n",
    "    print()\n",
    "    print(\"Actual labels:\\n--------------\")\n",
    "    print(corpus_tags_test[idx])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Disclaimer: Notebook is based on exercises of the Natural Language Processing Udacity Nanodegree."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
