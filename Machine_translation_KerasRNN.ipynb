{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Machine Translation \n",
    "\n",
    "We want to buld an end-to-end machine translation pipeline to translate English text to French, using different neural network architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import collections\n",
    "import numpy as np\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model\n",
    "from keras.layers import GRU, Input, Dense, TimeDistributed, Activation, RepeatVector, Bidirectional, Flatten, LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import sparse_categorical_crossentropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset\n",
    "Use a dataset that contains a small vocabulary (to be able to run it on a local machine). The data is partially preprocessed: the puncuations have been delimited using spaces and all text is lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    \"\"\"\n",
    "    Load dataset\n",
    "    \"\"\"\n",
    "    input_file = os.path.join(path)\n",
    "    with open(input_file, \"r\") as f:\n",
    "        data = f.read()\n",
    "\n",
    "    return data.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load English data\n",
    "english_sentences = load_data('data/small_vocab_en')\n",
    "# Load French data\n",
    "french_sentences = load_data('data/small_vocab_fr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "small_vocab_en Line 1:  new jersey is sometimes quiet during autumn , and it is snowy in april .\n",
      "small_vocab_fr Line 1:  new jersey est parfois calme pendant l' automne , et il est neigeux en avril .\n",
      "small_vocab_en Line 1301:  our least favorite fruit is the peach , but your least favorite is the lemon .\n",
      "small_vocab_fr Line 1301:  notre moins fruit préféré est la pêche , mais votre moins préféré est le citron .\n"
     ]
    }
   ],
   "source": [
    "for sample_i in [0,1300]:\n",
    "    print('small_vocab_en Line {}:  {}'.format(sample_i + 1, english_sentences[sample_i]))\n",
    "    print('small_vocab_fr Line {}:  {}'.format(sample_i + 1, french_sentences[sample_i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1823250 English words.\n",
      "227 unique English words.\n",
      "10 Most common words in the English dataset:\n",
      "\"is\" \",\" \".\" \"in\" \"it\" \"during\" \"the\" \"but\" \"and\" \"sometimes\"\n",
      "\n",
      "1961295 French words.\n",
      "355 unique French words.\n",
      "10 Most common words in the French dataset:\n",
      "\"est\" \".\" \",\" \"en\" \"il\" \"les\" \"mais\" \"et\" \"la\" \"parfois\"\n"
     ]
    }
   ],
   "source": [
    "# counts the unique words\n",
    "english_words_counter = collections.Counter([word for sentence in english_sentences for word in sentence.split()])\n",
    "french_words_counter = collections.Counter([word for sentence in french_sentences for word in sentence.split()])\n",
    "\n",
    "print('{} English words.'.format(len([word for sentence in english_sentences for word in sentence.split()])))\n",
    "print('{} unique English words.'.format(len(english_words_counter)))\n",
    "print('10 Most common words in the English dataset:')\n",
    "print('\"' + '\" \"'.join(list(zip(*english_words_counter.most_common(10)))[0]) + '\"')\n",
    "print()\n",
    "print('{} French words.'.format(len([word for sentence in french_sentences for word in sentence.split()])))\n",
    "print('{} unique French words.'.format(len(french_words_counter)))\n",
    "print('10 Most common words in the French dataset:')\n",
    "print('\"' + '\" \"'.join(list(zip(*french_words_counter.most_common(10)))[0]) + '\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "1) **Tokenize words to ids** \n",
    "\n",
    "2) **Padding** in the end of the sentence so that English and French sequences have the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(x):\n",
    "    \"\"\"\n",
    "    Tokenize x\n",
    "    :param x: List of sentences/strings to be tokenized\n",
    "    :return: Tuple of (tokenized x data, tokenizer used to tokenize x)\n",
    "    \"\"\"\n",
    "    # Updates internal vocabulary based on a list of texts. This method creates the vocabulary index based on word frequency\n",
    "    x_tk = Tokenizer()\n",
    "    \n",
    "    # Transforms each text in texts to a sequence of integers\n",
    "    x_tk.fit_on_texts(x) \n",
    "    return x_tk.texts_to_sequences(x), x_tk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad(x, length=None):\n",
    "    \"\"\"\n",
    "    Pad x\n",
    "    :param x: List of sequences.\n",
    "    :param length: Length to pad the sequence to.  If None, use length of longest sequence in x.\n",
    "    :return: Padded numpy array of sequences\n",
    "    \"\"\"\n",
    "    if length==None:\n",
    "        length = max([len(el) for el in x])\n",
    "    \n",
    "    # padding post adds zeros in the end of array\n",
    "    return pad_sequences(x, maxlen=length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(x, y):\n",
    "    \"\"\"\n",
    "    Preprocess x and y\n",
    "    :param x: Feature List of sentences\n",
    "    :param y: Label List of sentences\n",
    "    :return: Tuple of (Preprocessed x, Preprocessed y, x tokenizer, y tokenizer)\n",
    "    \"\"\"\n",
    "    preprocess_x, x_tk = tokenize(x)\n",
    "    preprocess_y, y_tk = tokenize(y)\n",
    "\n",
    "    preprocess_x = pad(preprocess_x)\n",
    "    preprocess_y = pad(preprocess_y)\n",
    "\n",
    "    # Keras's sparse_categorical_crossentropy function requires the labels to be in 3 dimensions\n",
    "    preprocess_y = preprocess_y.reshape(*preprocess_y.shape, 1)\n",
    "\n",
    "    return preprocess_x, preprocess_y, x_tk, y_tk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max English sentence length: 15\n",
      "Max French sentence length: 21\n",
      "English vocabulary size: 199\n",
      "French vocabulary size: 344\n"
     ]
    }
   ],
   "source": [
    "# apply preprocessing to dataset\n",
    "preproc_english_sentences, preproc_french_sentences, english_tokenizer, french_tokenizer =\\\n",
    "    preprocess(english_sentences, french_sentences)\n",
    "    \n",
    "max_english_sequence_length = preproc_english_sentences.shape[1]\n",
    "max_french_sequence_length = preproc_french_sentences.shape[1]\n",
    "english_vocab_size = len(english_tokenizer.word_index)\n",
    "french_vocab_size = len(french_tokenizer.word_index)\n",
    "\n",
    "print(\"Max English sentence length:\", max_english_sequence_length)\n",
    "print(\"Max French sentence length:\", max_french_sequence_length)\n",
    "print(\"English vocabulary size:\", english_vocab_size)\n",
    "print(\"French vocabulary size:\", french_vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post-processing\n",
    "From predicted token ids to French words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`logits_to_text` function loaded.\n"
     ]
    }
   ],
   "source": [
    "def logits_to_text(logits, tokenizer):\n",
    "    \"\"\"\n",
    "    Turn logits from a neural network into text using the tokenizer\n",
    "    :param logits: Logits from a neural network\n",
    "    :param tokenizer: Keras Tokenizer fit on the labels\n",
    "    :return: String that represents the text of the logits\n",
    "    \"\"\"\n",
    "    index_to_words = {id: word for word, id in tokenizer.word_index.items()}\n",
    "    index_to_words[0] = '<PAD>'\n",
    "\n",
    "    return ' '.join([index_to_words[prediction] for prediction in np.argmax(logits, 1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling\n",
    "We'll experiment with different model architectures: \n",
    "- 1: a simple RNN\n",
    "- 2: RNN with an embedding layer\n",
    "- 3: Bidirectional RNN with embedding "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 1: RNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
    "    \"\"\"\n",
    "    Build and train a basic RNN on x and y\n",
    "    :param input_shape: Tuple of input shape\n",
    "    :param output_sequence_length: Length of output sequence\n",
    "    :param english_vocab_size: Number of unique English words in the dataset\n",
    "    :param french_vocab_size: Number of unique French words in the dataset\n",
    "    :return: Keras model built, but not trained\n",
    "    \"\"\"\n",
    "    \n",
    "    learning_rate= 0.1\n",
    "    input_l = Input(shape=input_shape[1:])\n",
    "    # The simplest RNN consists of a dense layer with size Ninput (padded length so that input \n",
    "    # and output have the same shape) * french vocab size\n",
    "    rnn = Dense(french_vocab_size +1)(input_l)\n",
    "    # The output layer is a softmax layer that, for each possible french word and sequence position, tells what \n",
    "    # is the most likely word (so this works to predict single inputs)\n",
    "    model = Model(input_l, Activation('softmax')(rnn))\n",
    "\n",
    "    print(model.output_shape)\n",
    "    print(model.summary())\n",
    "    model.compile(loss=sparse_categorical_crossentropy,\n",
    "                  optimizer=Adam(learning_rate),\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshaping the input to work with a basic RNN\n",
    "# input is padded to have same size as output (21)\n",
    "tmp_x = pad(preproc_english_sentences, max_french_sequence_length)\n",
    "# Code below reshapes input to be (batch_size, sequence_length, output_dim)\n",
    "tmp_x = tmp_x.reshape((-1, preproc_french_sentences.shape[-2], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 21, 345)\n",
      "Model: \"model_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_8 (InputLayer)         (None, 21, 1)             0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 21, 345)           690       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 21, 345)           0         \n",
      "=================================================================\n",
      "Total params: 690\n",
      "Trainable params: 690\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 110288 samples, validate on 27573 samples\n",
      "Epoch 1/10\n",
      "110288/110288 [==============================] - 29s 262us/step - loss: 3.7188 - accuracy: 0.4232 - val_loss: 3.0970 - val_accuracy: 0.4345\n",
      "Epoch 2/10\n",
      "110288/110288 [==============================] - 29s 263us/step - loss: 3.0778 - accuracy: 0.4636 - val_loss: 3.0351 - val_accuracy: 0.4729\n",
      "Epoch 3/10\n",
      "110288/110288 [==============================] - 29s 259us/step - loss: 3.0519 - accuracy: 0.4732 - val_loss: 3.0740 - val_accuracy: 0.4755\n",
      "Epoch 4/10\n",
      "110288/110288 [==============================] - 30s 273us/step - loss: 3.1079 - accuracy: 0.4729 - val_loss: 3.1230 - val_accuracy: 0.4761\n",
      "Epoch 5/10\n",
      "110288/110288 [==============================] - 30s 275us/step - loss: 3.1463 - accuracy: 0.4724 - val_loss: 3.1625 - val_accuracy: 0.4704\n",
      "Epoch 6/10\n",
      "110288/110288 [==============================] - 29s 262us/step - loss: 3.1386 - accuracy: 0.4724 - val_loss: 3.1157 - val_accuracy: 0.4735\n",
      "Epoch 7/10\n",
      "110288/110288 [==============================] - 29s 263us/step - loss: 3.1194 - accuracy: 0.4734 - val_loss: 3.1049 - val_accuracy: 0.4736\n",
      "Epoch 8/10\n",
      "110288/110288 [==============================] - 28s 254us/step - loss: 3.0920 - accuracy: 0.4732 - val_loss: 3.0620 - val_accuracy: 0.4696\n",
      "Epoch 9/10\n",
      "110288/110288 [==============================] - 29s 260us/step - loss: 3.1304 - accuracy: 0.4729 - val_loss: 3.2616 - val_accuracy: 0.4735\n",
      "Epoch 10/10\n",
      "110288/110288 [==============================] - 28s 253us/step - loss: 3.2541 - accuracy: 0.4729 - val_loss: 3.2787 - val_accuracy: 0.4706\n",
      "est est est est en est est est est est est est est <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"
     ]
    }
   ],
   "source": [
    "# Train the neural network\n",
    "simple_rnn_model = simple_model(\n",
    "    tmp_x.shape,\n",
    "    max_french_sequence_length,\n",
    "    english_vocab_size,\n",
    "    french_vocab_size)\n",
    "\n",
    "simple_rnn_model.fit(tmp_x, preproc_french_sentences, batch_size=1024, epochs=10, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new jersey is sometimes quiet during autumn , and it is snowy in april .\n",
      "est est est est en est est est est est est est est <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "our least favorite fruit is the peach , but your least favorite is the lemon .\n",
      "est est est est est est aime est est est est est est chaud <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"
     ]
    }
   ],
   "source": [
    "# Print predictions of two sample sentences\n",
    "print(english_sentences[0])\n",
    "print(logits_to_text(simple_rnn_model.predict(tmp_x[np.newaxis, 0, :])[0], french_tokenizer))\n",
    "\n",
    "print(english_sentences[1300])\n",
    "print(logits_to_text(simple_rnn_model.predict(tmp_x[np.newaxis, 1300, :])[0], french_tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model seems to predict the most probable words most of the times. Playing a bit with the learning rate does not seem to help much.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 2: RNN with word embeddings\n",
    "We use an embedding to better represent words in vector representation (n-dimensional, with n = embedding size)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
    "    \"\"\"\n",
    "    Build and train a RNN model using word embedding on x and y\n",
    "    :param input_shape: Tuple of input shape\n",
    "    :param output_sequence_length: Length of output sequence\n",
    "    :param english_vocab_size: Number of unique English words in the dataset\n",
    "    :param french_vocab_size: Number of unique French words in the dataset\n",
    "    :return: Keras model built, but not trained\n",
    "    \"\"\"\n",
    "\n",
    "    learning_rate= 1e-3\n",
    "    input_l = Input(shape=input_shape[1:])\n",
    "    # Add an Embedding layer to smartly encode the english inputs\n",
    "    emb_size = 100\n",
    "    emb_layer = Embedding(english_vocab_size+1, emb_size)(input_l)\n",
    "    # using GRU with TimeDistributed vs Dense improves performance from 0.6 to 0.8!\n",
    "    rnn = GRU(64, return_sequences=True)(emb_layer)\n",
    "    logits = TimeDistributed(Dense(french_vocab_size+1, activation='softmax'))(rnn)\n",
    "\n",
    "    model = Model(input_l, logits)\n",
    "    model.compile(loss=sparse_categorical_crossentropy, \n",
    "                        optimizer=Adam(learning_rate), \n",
    "                        metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshaping the input to work with embeddings [input shape of embeddings should be (batch_size, sequence_length)]\n",
    "tmp_x = pad(preproc_english_sentences, max_french_sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_11 (InputLayer)        (None, 21)                0         \n",
      "_________________________________________________________________\n",
      "embedding_6 (Embedding)      (None, 21, 100)           20000     \n",
      "_________________________________________________________________\n",
      "gru_6 (GRU)                  (None, 21, 64)            31680     \n",
      "_________________________________________________________________\n",
      "time_distributed_6 (TimeDist (None, 21, 345)           22425     \n",
      "=================================================================\n",
      "Total params: 74,105\n",
      "Trainable params: 74,105\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 110288 samples, validate on 27573 samples\n",
      "Epoch 1/10\n",
      "110288/110288 [==============================] - 31s 277us/step - loss: 3.7385 - accuracy: 0.4033 - val_loss: 2.9149 - val_accuracy: 0.4093\n",
      "Epoch 2/10\n",
      "110288/110288 [==============================] - 30s 275us/step - loss: 2.5392 - accuracy: 0.4617 - val_loss: 2.0794 - val_accuracy: 0.5458\n",
      "Epoch 3/10\n",
      "110288/110288 [==============================] - 31s 279us/step - loss: 1.7254 - accuracy: 0.5987 - val_loss: 1.4392 - val_accuracy: 0.6507\n",
      "Epoch 4/10\n",
      "110288/110288 [==============================] - 32s 293us/step - loss: 1.2476 - accuracy: 0.6944 - val_loss: 1.0890 - val_accuracy: 0.7312\n",
      "Epoch 5/10\n",
      "110288/110288 [==============================] - 32s 294us/step - loss: 0.9804 - accuracy: 0.7543 - val_loss: 0.8825 - val_accuracy: 0.7731\n",
      "Epoch 6/10\n",
      "110288/110288 [==============================] - 31s 283us/step - loss: 0.8154 - accuracy: 0.7853 - val_loss: 0.7514 - val_accuracy: 0.7986\n",
      "Epoch 7/10\n",
      "110288/110288 [==============================] - 31s 280us/step - loss: 0.7067 - accuracy: 0.8082 - val_loss: 0.6620 - val_accuracy: 0.8194\n",
      "Epoch 8/10\n",
      "110288/110288 [==============================] - 31s 286us/step - loss: 0.6298 - accuracy: 0.8256 - val_loss: 0.5945 - val_accuracy: 0.8344\n",
      "Epoch 9/10\n",
      "110288/110288 [==============================] - 31s 282us/step - loss: 0.5715 - accuracy: 0.8390 - val_loss: 0.5450 - val_accuracy: 0.8448\n",
      "Epoch 10/10\n",
      "110288/110288 [==============================] - 31s 278us/step - loss: 0.5262 - accuracy: 0.8495 - val_loss: 0.5035 - val_accuracy: 0.8555\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f399f2dbb00>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the neural network\n",
    "embedded_model = embed_model(\n",
    "    tmp_x.shape,\n",
    "    max_french_sequence_length,\n",
    "    english_vocab_size,\n",
    "    french_vocab_size)\n",
    "\n",
    "embedded_model.fit(tmp_x, preproc_french_sentences, batch_size=1024, epochs=10, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new jersey is sometimes quiet during autumn , and it is snowy in april .\n",
      "new jersey est parfois calme en l' automne il il est neigeux en en <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "our least favorite fruit is the peach , but your least favorite is the lemon .\n",
      "notre fruit préféré moins est la pêche mais votre moins préféré est la citron <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"
     ]
    }
   ],
   "source": [
    "# Print predictions of two sample sentences\n",
    "print(english_sentences[0])\n",
    "print(logits_to_text(embedded_model.predict(tmp_x[np.newaxis, 0, :])[0], french_tokenizer))\n",
    "\n",
    "print(english_sentences[1300])\n",
    "print(logits_to_text(embedded_model.predict(tmp_x[np.newaxis, 1300, :])[0], french_tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding the Embedding layer and using a GRU with TimeDistributed layer already achieves quite a good performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 3: Embedding and Bidirectional RNNs\n",
    "The model incorporates embeddings and bidirectional RNN (so that the model can see following, and not only preceding, words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_final(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
    "    \"\"\"\n",
    "    Build and train a model that incorporates embedding, encoder-decoder, and bidirectional RNN on x and y\n",
    "    :param input_shape: Tuple of input shape\n",
    "    :param output_sequence_length: Length of output sequence\n",
    "    :param english_vocab_size: Number of unique English words in the dataset\n",
    "    :param french_vocab_size: Number of unique French words in the dataset\n",
    "    :return: Keras model built, but not trained\n",
    "    \"\"\"\n",
    "    learning_rate= 1e-3\n",
    "    input_l = Input(shape=input_shape[1:])\n",
    "    emb_size = 100\n",
    "   \n",
    "    X = Embedding(input_dim=english_vocab_size+1, output_dim=emb_size)(input_l)\n",
    "    # RepeatVector takes input and makes it fit the size you want. However, as we want to import all the info from Embedding,\n",
    "    # we need to Flatten first \n",
    "    X = Flatten()(X)\n",
    "    emb_layer = RepeatVector(output_sequence_length)(X)\n",
    "\n",
    "    # Bidirectional creates a set of 2*rnn size, one to look at input in foward direction, one in backward direction\n",
    "    rnn = Bidirectional(GRU(64, return_sequences=True))(emb_layer)\n",
    "    logits = TimeDistributed(Dense(french_vocab_size+1, activation='softmax'))(rnn)\n",
    "\n",
    "    model = Model(input_l, logits)\n",
    "\n",
    "    model.compile(loss=sparse_categorical_crossentropy, \n",
    "                        optimizer=Adam(learning_rate), \n",
    "                        metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshaping the input to work with embeddings\n",
    "tmp_x = pad(preproc_english_sentences, max_french_sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_13\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_13 (InputLayer)        (None, 21)                0         \n",
      "_________________________________________________________________\n",
      "embedding_8 (Embedding)      (None, 21, 100)           20000     \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 2100)              0         \n",
      "_________________________________________________________________\n",
      "repeat_vector_2 (RepeatVecto (None, 21, 2100)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 21, 128)           831360    \n",
      "_________________________________________________________________\n",
      "time_distributed_8 (TimeDist (None, 21, 345)           44505     \n",
      "=================================================================\n",
      "Total params: 895,865\n",
      "Trainable params: 895,865\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 110288 samples, validate on 27573 samples\n",
      "Epoch 1/10\n",
      "110288/110288 [==============================] - 322s 3ms/step - loss: 2.9846 - accuracy: 0.4249 - val_loss: 2.2429 - val_accuracy: 0.4642\n",
      "Epoch 2/10\n",
      "110288/110288 [==============================] - 349s 3ms/step - loss: 2.0219 - accuracy: 0.5022 - val_loss: 1.8569 - val_accuracy: 0.5319\n",
      "Epoch 3/10\n",
      "110288/110288 [==============================] - 365s 3ms/step - loss: 1.7321 - accuracy: 0.5653 - val_loss: 1.5850 - val_accuracy: 0.6072\n",
      "Epoch 4/10\n",
      "110288/110288 [==============================] - 365s 3ms/step - loss: 1.4169 - accuracy: 0.6431 - val_loss: 1.2802 - val_accuracy: 0.6769\n",
      "Epoch 5/10\n",
      "110288/110288 [==============================] - 356s 3ms/step - loss: 1.1678 - accuracy: 0.7006 - val_loss: 1.0704 - val_accuracy: 0.7192\n",
      "Epoch 6/10\n",
      "110288/110288 [==============================] - 349s 3ms/step - loss: 0.9970 - accuracy: 0.7364 - val_loss: 0.9299 - val_accuracy: 0.7519\n",
      "Epoch 7/10\n",
      "110288/110288 [==============================] - 343s 3ms/step - loss: 0.8778 - accuracy: 0.7634 - val_loss: 0.8403 - val_accuracy: 0.7731\n",
      "Epoch 8/10\n",
      "110288/110288 [==============================] - 357s 3ms/step - loss: 0.7894 - accuracy: 0.7864 - val_loss: 0.7503 - val_accuracy: 0.7973\n",
      "Epoch 9/10\n",
      "110288/110288 [==============================] - 361s 3ms/step - loss: 0.7262 - accuracy: 0.8023 - val_loss: 0.6949 - val_accuracy: 0.8095\n",
      "Epoch 10/10\n",
      "110288/110288 [==============================] - 360s 3ms/step - loss: 0.6472 - accuracy: 0.8273 - val_loss: 0.6211 - val_accuracy: 0.8348\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f399b780a58>"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the neural network\n",
    "final_model = model_final(\n",
    "    tmp_x.shape,\n",
    "    max_french_sequence_length,\n",
    "    english_vocab_size,\n",
    "    french_vocab_size)\n",
    "\n",
    "final_model.fit(tmp_x, preproc_french_sentences, batch_size=1024, epochs=10, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new jersey is sometimes quiet during autumn , and it is snowy in april .\n",
      "new jersey est parfois calme pendant l'automne et il est est en en <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "our least favorite fruit is the peach , but your least favorite is the lemon .\n",
      "votre fruit préféré moins est la chaux mais votre moins préféré est le citron <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"
     ]
    }
   ],
   "source": [
    "# Print predictions of two sample sentences\n",
    "print(english_sentences[0])\n",
    "print(logits_to_text(final_model.predict(tmp_x[np.newaxis, 0, :])[0], french_tokenizer))\n",
    "\n",
    "print(english_sentences[1300])\n",
    "print(logits_to_text(final_model.predict(tmp_x[np.newaxis, 1300, :])[0], french_tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a bidirectional layer does not seem to significantly improve model performance. The long training time of this network, however, prevents further investigation on possible improvements resulting from parameter tuning and modifications of the network structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Disclaimer: This notebook is inspired by a project which is part of the Natural language Processing Udacity Nanodegree."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
